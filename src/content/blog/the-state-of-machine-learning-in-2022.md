---
pubDate: "2022-04-18"
title: The State of Machine Learning in 2022
---

I’ve been passionate about machine learning for as long as I can remember. I built my first robot more than 20 years ago
out of Lego Mindstorm bricks. It’s been a wild ride since then, and the field keeps evolving. Running a business that is
closely tied to the progress of state-of-the-art machine learning means I’m trying to stay up to date with what is going
on. In this post, we will go through what I consider the most interesting breakthroughs recently. **We will cover
embeddings, attention, transformers, and multi-modal models.**

**At the end of the post, I will share some thoughts on what this means for society and business.** If you don’t want
all the technical details, you can skip to the end now. But I encourage you to actually try to understand the recent
breakthroughs. Otherwise, you will have a hard time determining what these breakthroughs mean for you and your business.

![DALLE2. Image generated by an algorithm based on caption provided by a human.](https://images.squarespace-cdn.com/content/v1/5c2b12dae17ba3d4ccb3bdfd/7c21fbc9-ca39-41b6-afb0-07b319d7f360/Screenshot+2022-04-20+at+11.03.01.png)
**Caption:** Teddy bears working on new AI research on the moon in the 1980s

### Learning requires efficient abstractions

Machine learning, in my opinion, is about finding efficient abstractions that enable the robust interpretation of
diverse data. The “problem” with reality is that most things are rare. If you look too closely at each data point it
appears unique. You have to “squint” to cope with all the information we are exposed to. Humans are great at this. We
half-ass most data processing and make heavy use of prejudice in the name of efficiency. This ability has served us well
in our effort to process diverse data in an energy-efficient way. We are optimized to minimize learning time and energy
consumption while maximizing procreation. It is less good if you want a consistent and fair evaluation of data. Humans
are bad at consistency and fairness.

Think of it this way: if we always considered each situation as a completely new situation based on minor changes we
would get exhausted. Instead, we remember previous situations we’ve been in and abstract away the specific details. “Oh,
I’m approaching an intersection now, I know there can be cars coming from different directions. Sure, it is a slightly
different intersection than before but I still have some idea of what might happen.”

The challenge for machine learning researchers is to figure out how to extract the most efficient abstractions. If they
are too crude, the quality of output goes down. If they are too exact, learning becomes very, very expensive. I equate
these abstractions to “common sense”.

Embeddings were my first real machine learning “mind-fuck” experience. I still
remember [reading Mikolovs paper](https://arxiv.org/pdf/1310.4546.pdf) the first time and realizing what a huge thing
the embedding concept was going to be. It’s so simple yet so elegant. I won’t spend much time on it here since it’s old
news, but it still forms the foundation for so many things, so you better make sure you understand this concept. A good
way to learn is to
watch [this presentation from Google](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,like%20sparse%20vectors%20representing%20words.).
Let’s say we have 10.000 words and want to represent a word as a vector. One option would be to replace the word with a
vector of dimension 10000 with all cells set to zero except the one representing our word. Problem is, you would get a
lot of very large vectors with mostly zeros in them. Instead, we can create lower dimensional vectors representing each
word. If you create these vectors based on the context in which words ocrrus in it turns they preserve semantic
properties and open up for using linear algebra on them in cool ways (like in the picture below). Learn about
embeddings.

![](https://storage.googleapis.com/langkilde-se-images/672a00d7-ea8d-4c5f-a6ae-db9e50bd5cf6.jpeg)

### Attention and Transformers

In recent years, the most influential paper in my opinion
is “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762v5.pdf)” from the Brain at Google. This paper
demonstrated that attention mechanisms can replace
both [recurrent](https://en.wikipedia.org/wiki/Recurrent_neural_network)
and [convolutional](https://en.wikipedia.org/wiki/Convolutional_neural_network) neural networks, while still both
improving results and lowering the cost of training. After years of just pushing the limits of RNNs and CNNs, a new
paradigm emerged. RNNs had the downside that they did not allow for parallelized training as each new prediction was
predicated on the previous prediction. Some tried to overcome this limitation
by [turning sequences into “images”](https://arxiv.org/pdf/1705.03122.pdf) and then applying CNNs instead. The problem
with CNNs is that they have a hard time learning distance relationships as “signal strength” diminishes with distance.
Enter the Transformer.

![](https://storage.googleapis.com/langkilde-se-images/90d0d7dd-b1b3-47b8-b6af-34fda1c81d44.jpeg)

The Transformer follows the overall architecture of [encoder/decoders](https://arxiv.org/pdf/1406.1078.pdf).
Transformers process sentences in the form of a sequence of embeddings that it learns during training. Each token in the
sentence is represented by a vector (in the original paper they use 512-dim vectors)
with [positional encoding](https://huggingface.co/blog/encoder-decoder) added, i.e. the vector also indicates the
position of the word in the sentence. The Encoder learns a representation of the input data (encoding) based on which
the Decoder produces the target output. This pattern provides a dimensionality reduction step, which goes back to the
idea of finding abstractions. Transformers only pass on the information necessary for decoding from the input to the
encoding.

![](https://storage.googleapis.com/langkilde-se-images/e9b1de67-9316-4836-a63d-6b3841448607.jpeg)

Both the Encoder and Decoder make use of **attention**. So what is attention? The idea behind attention is to replace
embeddings with _better_ embeddings that also contain information about the context in which the word appears.
Understanding how it works requires linear algebra. The easiest to understand
is [scaled dot-product attention](https://paperswithcode.com/method/scaled). Let’s assume we have an input sentence
X = [x_1…x_n] where each word x is an embedding. Our goal is to replace the word x_n with a new vector y_n. We do this
by summing over all the words in the sentence, with each word weighted by some number. The self-attention layer
introduces two matrices, W_Q and W_K by which we transform the words before we combine them. The interpretation of these
matrices is not obvious. We want to put numbers into these matrices so that the model considers the right context words
when interpreting a specific word.

![](https://storage.googleapis.com/langkilde-se-images/b5261eb9-c7cc-4059-afd2-1fdd58566943.jpeg)

Prof. Lennart Svensson [has a great lecture explaining this](https://www.youtube.com/watch?v=0SmNEp4zTpc). We know that
words that are related cluster with respect cosine distance of their word embeddings. One way to interpret the W_K
matrix is that we need to select it so that it forms an identity matrix in relation to which the key vectors are
identical to the original word embeddings. The keys would preserve the original similarity between words at face value.
If we then select W_Q so that our new embedding points in the same direction as the specific words we want the model to
care about, their inner product will then be large.

![](https://storage.googleapis.com/langkilde-se-images/bb928c2c-bd19-4770-9a9d-576227ebcafb.jpeg)

**I think of attention this way:** attention means learning parameters that are used to create new embeddings for input
words that contain information about the context. All of the parameters that fill these matrices are learned
simultaneously. That means that embeddings and attention, along with all other parameters, are selected so that they
together maximize the quality of the network. This gives the model enormous expressive capacity, at the price of a huge
number of parameters and a huge cost of computation.

Transformers use Multi-Head Attention which means that it uses multiple queries per word rather than just one. The
reasoning is that words could have different meanings depending on context. The model uses these multi-headed attention
mechanisms in multiple ways. One of these is referred to as “self-attention”. The idea behind self-attention is that the
word itself impacts its own meaning and that the most appropriate word embedding depends on the context. I won’t go
through all of the details. The original Transformers paper has a nice illustration of the output weights of these
attention heads for various sentences. As you can see from the first attention head, the weights
resemble [dependency parsing](https://web.stanford.edu/~jurafsky/slp3/14.pdf), which I guess is exactly the point.

![](https://storage.googleapis.com/langkilde-se-images/566ea5f4-f02c-4278-87c0-91fed6c2b592.jpeg)

Anyway, this is just the first step of the Encoder block. We feed the input sentence as a set of vectors with positional
encoding, and we get a set of weights back that describe how each token should be regarded based on the entire sentence.

![](https://storage.googleapis.com/langkilde-se-images/bd2aef82-55f2-4750-9751-0a5732653634.jpeg)

We add residual connections that carry over previous embeddings to subsequent layers, i.e. we mix together the original
embeddings with the information learned from the multi-head attention mechanism. Finally, we add some layer
normalization, and voila we have our basic Encoder block. The transformer then uses six of these stacked, with the
output from the last block serving as input to the Decoder.

![](https://storage.googleapis.com/langkilde-se-images/3aa7603f-cbd8-4b8d-9a5e-32486926925e.jpeg)

The Decoder is similar to the Encoder. One major difference is that the attention mechanism is “masked”, which means it
gets a gradually increased visibility of the input sentence. This makes sense since we cannot time travel. Another
difference is that it treats its most recent output as the last token of its input.

![](https://storage.googleapis.com/langkilde-se-images/8aa0a4d7-68a4-4a34-b258-b0e35daef331.jpeg)

This kind of processing is called auto-regressive. A nice consequence of this is that the output can have a different
length than the input. The Decoder ultimately outputs a vector the size of our known vocabulary, and the softmax layer
converts that into a vector of probabilities. It might seem simple to then just pick the most probable word, but it
turns out using something called beam search produces even better results. Anyway, the predicted word is fed back to the
decoder as the last part of the next decoder input. This process continues until we predict the end-of-sentence token.

In the end, we have a construction that assumes no recurrence or convolutions when processing the input data. As long as
we can express our input as sequence data, we can apply this approach even in computer vision or reinforcement learning.
The core idea is to enrich the embeddings with features from the global context.

If you want an even better explanation of all of this, I highly
recommend [Prof. Lennart Svensson’s lecture](https://www.youtube.com/watch?v=0SmNEp4zTpc).

![](https://storage.googleapis.com/langkilde-se-images/b7b48715-f3a3-4959-8085-c48460b0f8ab.jpeg)

### Few-Shot Learners such as GPT-3

In May of 2020 researchers from OpenAI described the development of GPT-3 in the
paper “[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165v4.pdf)”. The big breakthrough in GPT-3
is that it removes the need for huge task-specific labeled datasets in order to learn specific tasks, assuming analogous
tasks are already represented in available language datasets (e.g. all of the internet). To quote from the paper:

> …humans do not require large supervised datasets to learn most language tasks – a brief directive in natural
> language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of
> demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often
> sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to
> a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to
> seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy
> dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.

In order to achieve this, they build on the capacity of transformers. There is a race going on where the number of
parameters in language models is growing almost as fast as transistors. Each increase has brought improvements to all
sorts of NLP tasks, and there is evidence that they get better and better the larger they get.

![](https://storage.googleapis.com/langkilde-se-images/b84e1a84-9ae6-4543-ac53-5d2dacc54bc5.jpeg)

In the paper the authors describe four different settings in which they want to evaluate their model. This is relevant
to understand the title of the paper, and more importantly, when it works and not.

![](https://storage.googleapis.com/langkilde-se-images/cda92869-531e-4abe-86fc-500439be5e45.jpeg)

The title of the paper refers to “Few-Shot Learners” as it is described in the image above. The model architecture they
use is basically the same as in several earlier papers such
as “[Language Models are Unsupervised Multitask Learner](https://arxiv.org/pdf/2005.14165v4.pdf)”
and “[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)” (
the latter is the paper that coins the term GPT, which is short for Generative Pre-Training). The following figure
describes the basic idea:

![](https://storage.googleapis.com/langkilde-se-images/eebb28a3-0257-47e4-a825-f7c070a7cfb8.jpeg)

GPT uses a generative pre-training stage in which a language model learns language and then adds a supervised
fine-tuning stage in which the pre-trained model is adapted to a target task. The amount of examples used defines the
degree of fine-tuning (zero, one, few, etc). The Transformer architecture serves as the backbone of this pre-training
stage. Here is how the authors describe the setup:

![](https://storage.googleapis.com/langkilde-se-images/1ec6417e-5d0e-4aaf-b676-c016961dbcdc.jpeg)

So, they are learning to predict words using Transformers. Notice the use of “unsupervised corpus”. Personally, I think
this is misleading. The corpus is unsupervised in the sense that no one explicitly labeled the meaning of words in the
corpus. But it is supervised in the sense that words have been put into sentences by humans, i.e. someone always needs
to provide structure somehow.

Anyway, since the model is trained on continuous text, and not for example questions and answers, they apply some tricks
to get data into shape.

![](https://storage.googleapis.com/langkilde-se-images/2af322d4-3070-4f82-b387-c0d8f1f6f785.jpeg)

This setup is improved on a bit for later versions of GPT, but GPT-3 follows the same pattern. They then feed these
learning beasts huge amounts of text:

![](https://storage.googleapis.com/langkilde-se-images/acaf9b09-5825-4cee-894b-420c86132ca9.jpeg)

That’s a lot of human knowledge right there. This is where the supervised learning part comes in. In order for GPT-3 to
work, we need to have access to enormous amounts of documented human knowledge. Whatever the model knows is a result of
what is in these 300 billion tokens of knowledge. Fun fact: a challenge for the authors of the paper was to filter out
all of the test data from this corpus since most data is somewhere on the internet. The crazy thing that happens when
you put this gigantic model to work is that it can generate long sequences of text that read as if written by a human.
And all it needs is a few words to get started. For example, news article generation:

![](https://storage.googleapis.com/langkilde-se-images/9e5c2644-f828-483a-a246-1ae840d89bdb.jpeg)

To evaluate they asked humans to determine if a news article was written by a human or by GPT-3. Results show that
humans have a pretty hard time distinguishing real from robot.

![](https://storage.googleapis.com/langkilde-se-images/2e15223d-cc5b-4467-8fcd-a4610b33fab8.jpeg)

So how is this possible? By now you should have some idea. By training a huge language model on a huge corpus of text we
get a model that learns how to interpret words based on their context. Using that model, and a few tokens to give the
model a starting point, it can then predict suitable next words. The attention mechanism gives the model a strong
“memory” that lets it keep track of context.

### Multi-Modal Models

A modality is the type of channel used to communicate such as through images or sound. Humans rely on multi-modal input
in order to navigate the world. We hear, see and feel. This is an obvious source of inspiration for researchers. Before
we jump into state-of-the-art multi-modal models we will review how the pre-training concepts from GPT have been applied
to visual data.

The first paper I recommend
is “[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)”. This
paper builds on the breakthroughs with autoregressive and masked language models and describes what the authors call
CLIP (Contrastive Language-Image Pre-training).

![](https://storage.googleapis.com/langkilde-se-images/d56e312d-73bc-40ff-aa7e-fb420a9ef75a.jpeg)

The idea behind the paper is to learn directly from raw text about images, rather than learn predefined object classes.
Typically when training object detection models we define a set of classes and then proceed to label each object with
one of these classes. Besides being very tedious, this can also be limiting as not all objects fit into obvious classes.
So instead, the paper describes how to learn the connection between an image and its caption. CLIP is an efficient
method of learning this that is similar to GPT that is able to predict the text snippet based on an image.

More recently, models like [DALLE2](https://openai.com/dall-e-2/) have become the darling of the internet for their
ability to generate realistic images and art from a natural language description. The mechanics of DALLE2 are described
in the paper “[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf)”.
The technique relies on a CLIP latent space but learns an inverted version of CLIP, i.e. a decoder (or “unClip”). The
resulting decoder is a non-deterministic function that can generate images given a caption. The encoding/decoding
process generates, similar to GANs, semantically similar images. An even cooler aspect is that you can semantically
modify images by moving in the direction of any encoded text vector. This image gives a high-level overview of the
unClip process.

![](https://storage.googleapis.com/langkilde-se-images/d4936576-92af-41bb-8b64-ba1ade253f61.jpeg)

The results are very, very cool. For example, you can naturally blend styles by interpolating the CLIP image embedding
of different images and decoding the vectors.

![](https://storage.googleapis.com/langkilde-se-images/c66ff7f4-12f7-4e26-bec9-2dca5490ebad.jpeg)

![](https://storage.googleapis.com/langkilde-se-images/ade90f01-b3a0-4911-a263-026e21299558.jpeg)

The reason this sort of continuous interpolation is possible is that the images and text are both embedded in the same
latent space. That allows us to apply language-guided image manipulation. You can also write a text into a prompt and
get an image back. The decoder is based on diffusion models to produce images conditioned on CLIP image embeddings and
optionally text captions. In order to get high-resolution images, they train diffusion upsampler models. A few more
tricks are applied to make results even better, but essentially it’s CLIP encoding and then diffusion-based unClip. You
can find the details in the paper.

### Conclusions

So what does all this cool progress mean for humans? Or for businesses that support teams training machine learning
models? I think a few things are clear:

* **Foundational models will be considered commodity infrastructure.** The huge, underlying language and image models
  that are part of GPT and CLIP will be available from a few major companies like Google, Microsoft, Amazon, etc. At
  least until these models can be trained at a reasonable cost. I don’t think ML Engineers will train models the way we
  have until now for much longer. Instead, they will leverage existing infrastructure.

* **Foundational models do not mean we are on the brink of artificial general intelligence.** They are super cool, and
  clearly, computers can now learn a lot of nuance in a robust way. But in my opinion, independent competency will
  require combining these concepts with ideas from reinforcement learning and an ability to interact with our
  surroundings. Personally, I think a robot will need a perception system based on some future, transformers-like
  architecture (I expect we will find many clever abstractions in coming years) that learns from raw visual feeds and
  connects that to a reward-based system (like Q-learning) to achieve some seeded goal _could_ result in seemingly
  intelligent robots. It’s not pure science fiction anymore, but GPT-3 and DALLE2 do not mean humans are obsolete. Not
  yet anyway.

* **Whatever shitty opinions and biases are present in the foundational model will be there in the output.** The
  internet is full of crap. That means there will be a lot of undesirable bias buried in these foundational models.
  Uncovering such bias, and mitigating it, will be an important part of deploying models responsibly.

* **ML Engineers will tune foundational models to specific applications using specific datasets.** Rather than build
  huge datasets from scratch, ML Engineers will tune foundational models to specific use-cases. Humans are sloppy, so
  the foundational models trained on all human knowledge will be full of mistakes and bad decisions. For autonomous
  mobility specifically, I expect proprietary data to dominate for the time being, and that it is collected with the
  intention of making GPT/CLIP-like few-shot learners possible for driving specific tasks. I assume all the big players
  are working on something that essentially amounts to a generalized multi-modal model for driving. Leveraging
  foundational models in this work will drastically reduce the amount of labeled data required since you can avoid
  annotating things already mastered by some foundational models. It won’t be enough for releasing a safe product since
  you still need to make sure that you do not have a lot of strange stuff in the foundational model, but it will lower
  the cost of getting to the end goal a lot. Arguably, specialized ground-truth companies already provide this sort of
  service by automating ground-truth production based on pooled datasets.

* **Safety-critical applications will require detailed validation to find and stop bias and bad judgment.** Humans are
  not going away when developing and validating safety-critical systems. If you’ve read all of the papers linked above
  you will see that these models make hilarious mistakes. Mistakes will be fewer with larger datasets and more
  parameters, but they will still occur. Applying these powerful foundational models will require great attention to
  detail for many years to come. For autonomous mobility, I think fine-tuning to various tasks and gathering data for an
  ever-increasing operational design domain will be the challenge. Eventually, we will pack all the human knowledge
  required to drive into a huge neural network. But until then, we have to do a lot of checking and measuring. Even
  humans make mistakes after all.
---
pubDate: "2023-04-25"
title: Anthropocentrism, Alignment, and Personalization
---

# **Executive Summary**

* **First of all, progress in AI is predominantly positive** üéâ üöÄ We should enjoy the benefits of advanced machine  learning. The risk of a run-away scenario is most likely minimal since the likelihood of  a [strong discontinuity](https://pashanomics.substack.com/p/an-ai-realist-manifesto-neither-doomer?ref=thediff.co) in  capabilities is small.

* That said, I consider it a moral axiom and obligation to protect the subjective, conscious experiences of humans.

* Humans are valuable, but human existence is not unconditional.

* We impose regulation when self-interest fails to protect our long-term, common good.

* Super-intelligent AIs might be similar to companies, which implies they could be regulated.

* There is an old debate about whether technology risks replacing humans that is accelerating again. It is not clear if  ‚Äúthis time is different.‚Äù

* Human goals, preferences, and ethics are constantly changing. This protects us from automation.

### Anthropocentrism and Human Importance

I follow closely the AI debate currently unfolding. The most extreme part of the debate is about whether AI is anexistential threat to humans or not. This post explores how we have tried to protect the long-term wellbeing ofhumanity, and what might make sense now. Humans have a long history of favoring humans. Already in The Book of Genesis,which could be as old as 3400 years, you can read in verse 1:26:

> ‚Ä¶ and God said, let us make [humans] in our image, after our likeness: and **let [humans] have dominion** over the> fish of the sea, and over the fowl of the air, and over the cattle, and **over all the earth** , and over every creeping> thing that creepeth upon the earth.

I think any discussion about the alignment of human ethics and AI needs to start with the question: **are humansimportant?** While the answer might feel like an unmistakable YES, it likely isn‚Äôt a binary question for most people;it‚Äôs a matter of degree. Most humans probably think humans should exist. Any other position would legitimize genocideor, at the very least, suicide. And by reading this, you‚Äôve chosen to be alive, which means something.

## Alignment and The Common Good

While most of us want to exist, most humans also acknowledge the tension between our material expectations and theenvironment. The more possessions we want, the more strain we put on the environment. To manage this tension, **weimpose regulatory limits.** Even if we think humans should exist, our existence is not unconditional. Economists talkabout [The Tragedy of the Commons](https://en.wikipedia.org/wiki/Tragedy_of_the_commons):

> ‚Ä¶ a phenomenon in which common resources to which access is not regulated by fees based on individual users tend to> become depleted. If users of such resources act to maximize their self-interest and do not coordinate with others to> maximize the overall common good, exhaustion and even permanent destruction of the resource may result‚Ä¶

Most humans do not feel good about burning oil and coal, cutting down forests, causing the extinction of endangeredspecies, polluting coral reefs, or any of the countless other destructive things we nevertheless keep doing. Through ourconsumption, we contribute to exhausting Earth‚Äôs ecosystem. In the absence of decentralized **alignment** , we askgovernments to coordinate. Governments attempt to align personal needs with the long-term, common good.

## AI Alignment and Regulation of Companies

Recent progress in AI has sparked a discussion about the possible impact super-intelligent AI might have on humanity. Aterm that frequently occurs in this discussionis ‚Äú[AI alignment research](https://en.wikipedia.org/wiki/AI_alignment#:~:text=In%20the%20field%20of%20artificial,it%20advances%20the%20intended%20objectives.)‚Äù:

> ‚Ä¶alignment aims to steer systems toward humans‚Äô intended goals, preferences, or ethics

There are striking similarities between such research and the ultimate purpose of regulation. Government regulation aimsto steer markets toward humans‚Äô intended goals, preferences, or ethical principles, i.e., to avoid tragedies of thecommons where individual greed depletes essential common resources.

At the receiving end of regulation is the free market. Of all the forms for creating value we‚Äôve tried, free democraticmarkets powered by independent academia have been our most significant source of progress. So whatever regulation weimpose, it needs to be carefully balanced with the value of a free market. Most current AI regulation is a joke. And,while I‚Äôm at it, GDPR is a complete waste of money and time that makes Europe drastically less able to compete. Iconsider it my duty to avoid implementing any compromises due to GDPR. I share this to make sure you do not mistake myreasoning in this post as pro ‚Äúbig government.‚Äù I think a small government is probably better in most cases, but Idefinitely think we need governments.

Anyway, central to free markets are independent companies. Companies have a very straightforward, well-defined objectivefunction: **maximize shareholder value**. In a way, **companies operate as distributed, super-intelligent AIs**. Peoplework together to perform calculations and take actions that strive to capture as much value as possible.

It turns out that regulation of markets can help us escape certain local minima that, if not escaped, would depletecommon goods. Take electrification as an example: Tesla hasreceived [billions in subsidies](https://www.latimes.com/business/la-fi-hy-musk-subsidies-20150531-story.html), andbuyers of Tesla cars haveenjoyed [aggressive tax benefits](https://www.reuters.com/business/autos-transportation/tesla-pushes-norways-ev-sales-new-record-2021-10-01/).As a taxpayer, I‚Äôm happy to contribute to getting us off our oil addiction faster. While companies would likely haverealized the necessity of transitioning to electric vehicles anyway, government action undeniably accelerated ourtransition. Most industries welcome regulation. Pharmaceutical companies, for example, want a level playing field. Ifanyone could launch a drug without rigorous testing, the market would be impossible to navigate for doctors andpatients. A messy pharma market would ultimately hurt drug companies.

### The Debate: Will Technology Replace Humans?

I see a connection between companies in a free market, the tragedy of the commons, and AI. Let me explain: Due to theirobjective function, companies are naturally focused on efficiency and automation. Often, that boils down to removinghumans from processes. Less manual effort means more profitability. **An extreme outcome of this work would be thathumans are no longer needed. Is such an outcome an expression of human goals, preferences, and ethical principles?**

I see at least three possible lines of reasoning when responding to this question:

**1\. AGI is distant, and while AI is getting more capable, technology-driven unemployment isn‚Äôt something we need to beconcerned about.** Fears about new technology replacing humans are not new. The Luddites were a secret oath-basedorganization of English textile workers in the 19th century who formed a radical faction that destroyed textilemachinery. Each such historical scare has been proven wrong by consistent economic growth. Marc Andreessenhas [a detailed post on this topic](https://pmarca.substack.com/p/why-ai-wont-cause-unemployment). He isn‚Äôt exactlyneutral, but he makes a good point. His post argues that technology is already illegal in many sectors, such ashealthcare, education and housing, and that the result is a steady increase in cost. Sectors in red in this graph areheavily regulated, while sectors in blue allow for free competition. Turns out technology and competition has a strongdeflationary impact, without actually causing unemployment.

![](https://storage.googleapis.com/langkilde-se-images/6b144d50-2ef9-44d1-acc5-ee458ab07e7f.jpeg)

**2\. AI is getting more capable, and we risk removing many jobs that provide people with meaning. Our pursuit of profitdoes not stand above protecting the subjective human experience of purpose.** Lex Friedman interviewed Max Tegmarkrecently. Tegmark made a lovely case for why it is wrong for humanity to remove meaningfulwork. [Tegmark references](https://youtu.be/l3RaPJUhqkM?t=64) a great article by Scott Alexandertitled ‚Äú[Meditations on Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/).‚Äù Moloch is agame-theoretic monster that traps people in a race toward an outcome we ultimately do not want. I think Moloch is real,and that regulation can help people avoid ending up serving Moloch. It is possible that current AI developmentaccelerates to the point that most creative professions become financially worthless. I.e. it is possible thisunemployment scare is different. But we do not know. Not yet anyway.

**Related to this is an interesting question:** does work constitute a common good that we should or need to protect? Isit possible that AI developers could deplete the need for human work by maximizing their self-interest? I think thiswill be debated a lot in the coming years. Personally, I‚Äôm an optimist. I think we will find new and creative jobs thatreplace the ones we loose. But I‚Äôm sympathetic with the people who will need to find new jobs. We should help thosepeople transition.

**3\. AI risks evolving into AGI, which, if not aligned properly, could cause massive problems for humanity. As aresult, we need to slow down until we have alignment figured out.** The ‚ÄúDoomers‚Äù aren‚Äôt worried about such trivialissues as the preservation of meaningful work. They are concerned about the risk of AGI causing the extinction ofhumanity. They argue we should stop, or at least slow down, the development of AGI until we have figured out to preventhuman extinction. If you are indeed worried about this, then anything we can do to stop AGI from causing extinction isfair game, including, but not limited to, regulation.

### Humans‚Äô Goals, Preferences, and Ethics Evolve

Our best defense against automation is that we do not know the meaning of life. I mentioned alignment as the process ofsteering systems toward humans‚Äô intended goals, preferences, and ethics. I think discussing our purpose is an eternalactivity. Humans have always looked at the sky and asked ourselves: ‚ÄúWhy are we here?‚Äù Culture is built on mystery, anddoubt sits at the heart of the human condition. We do not know why we are here, and that‚Äôs great. That means there willalways be things to explore. We cannot automate a process for which we do not know the objective function.

This uncertainty has resulted in ideologies, beliefs, religions, doctrines, ethics, and other frameworks. Or in anotherword, culture:

> Culture is an umbrella term that encompasses the social behavior, institutions, and norms found in human societies, as> well as the knowledge, beliefs, arts, laws, customs, capabilities, and habits of the individuals in these groups. **Acultural norm codifies acceptable conduct in society**.

Since there is no single axiomatic purpose for human existence, humanity has developed a vast range of differentcultures, each providing its own version of ‚ÄúWhy‚Äù. There is a fascinating discourse about ‚ÄúAre GPT models biased ornot?‚Äù. Of course, they are. They reflect the cultural norms present in the training data. It is impossible to eradicatebias, since we cannot sterilize the human experience and wash away culture. We are one with culture. Instead, a morefeasible solution is personalization. Just as there are different newspapers, books, and social circles, we will wantAIs that reflect our worldview.

### The Subjective Human Experience is Valuable

This is not a prediction or speculation. This is a value statement. I think humans are _more_ important than machines. Iconsider this a moral axiom. I cannot prove why this is true. I just decided it is. I have decided that the subjectivehuman experience of consciousness is valuable and should be protected. Every time humans have questioned this principle,disaster has ensued, like the Holocaust. We have a moral obligation to preserve other humans‚Äô subjective experiences.And we should strive to maximize the happiness of all humans. If, at one point, automation or even AGI threatens thelong-term safety of humans, then I choose humans over machines. I‚Äôd much rather shut down AI than risk wiping outhumanity, but I don‚Äôt think there is any reason to shutdown AI progress based available data.

Instead, we should let AI technology flow as freely as possible and put a minimum amount of regulation in place. Weshould only regulate if companies risk depleting common goods. I don‚Äôt think we risk depleting our repository ofmeaningful work, so right now I doubt much regulation is meaningful.
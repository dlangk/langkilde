---
pubDate: "2026-02-24"
title: Navigating the Token Abundance
---

I regularly write "Company Update" emails to the team at Kognic. It's probably no surprise that most of these emails are focus on the impact of AI. Here is my latest email:

---

## AI: This Time is Different

Over the past three weeks, I've built more with Claude Code than I've done by hand in the last five years. Granted, this was on a "synthetic" project (my Yatzy workbench), but we are already seeing the same technology reshape our Engineering team. Most team members have already fully adopted agentic coding methods, and the rest are in the process of getting there.

We've also made it crystal clear: *We expect everyone to adopt Claude in their workflows before this summer.*

In the past, I've deliberately avoided being hyperbolic about how AI will transform work. I've felt that most AI lab narratives exaggerated actual model performance. *Not anymore. The technology is ready for broad adoption now.*

At Kognic, we choose to embrace this change. We succeed because we constantly stay at the frontier of technology. We adapt as new tools emerge, using them as a superpower. But adapting will still require significant work.

In this email, I want to reflect on some of the challenges we are likely to face now. The next three years will be emotionally demanding. We all need to prove ourselves as lifelong learners. Gaining superpowers does not mean stopping effort; it means increasing your leverage. If you start delegating your thinking to LLMs, you will quickly become useless. The secret is to fuse AI into your workflow so you can amplify your own power and contribution. Here is what I think matters as we learn:

## Separating Truth from Bullshit

The most challenging part of our new reality is that text has never been worth less. We live in an age of token abundance. Content flows from every endpoint at an unprecedented rate. I feel my brain working overtime, processing everything that comes at me from morning to evening. There is just too much text. Chatbots are amazingly powerful, but so fucking talkative. I now understand how you all feel when I get started... ðŸ˜…

This means we need personal frameworks to quickly detect bullshit. I've been building mine, and I want to share some of the tools I use. Feel free to steal them:

- *Good explanations are hard to vary.* This comes from David Deutsch and his book The Beginning of Infinity, and it's become my favorite filter. A good explanation is one where every detail serves a purpose. It should be impossible to replace or remove parts without destroying the explanation's ability to explain. Another way to put this: if your argument can justify anything, it implies nothing. Try writing your argument as a fill-in-the-blank template. If it can justify multiple contradictory conclusions, the argument is probably worthless. LLMs are terrible at this. They produce chains of reasoning that look plausible, but if one key assumption is wrong, the conclusion goes off the rails.

- *Good explanations are falsifiable.* They make specific claims that can be tested and, if necessary, rejected. Classical Popper. Can you express a null hypothesis? If not, you're probably not saying anything meaningful.

- *Good explanations are Why-Focused.* They explain the underlying mechanism, not just the pattern. My mom used to work in drug safety. The FDA won't let you sell a drug unless you can argue clearly why it works. It's not enough that it's effective; you need to understand the mechanism. I think we need to apply this standard to business reasoning, too.

- *Gather independent, first-hand information.* I keep coming back to this. The best example: run Deep Research on the data annotation market and read what comes out. It's useless crap. That's revealing. It gives me reason to think a lot of LLM-powered research is actually crap, it's just that I can't always tell. The reason is obivous: LLMs take whatever they find, however little there is, and turn it into a plausible-sounding story. They treat everything on a company's website as true. They treat market research reports as true regardless of source. They assume all search results are relevant. Anyone who has used the internet knows this is completely false. Deep Research works for stable things, like math and physics. LLMs are great at packaging knowledge. But as always, the "shit in, shit out" rule applies. Evaluating source credibility has never been more important.

- *Actively seek information that disproves your beliefs.* This is one of the hardest things to do in life. If you are uncertain or afraid of something, push harder on it. Go deeper. Ask more questions. Truth is always better than ignorance, even if it's short-term uncomfortable. Kognic survives because we face every challenge and setback with an open mind. Smart, successful people are bloodhounds for bullshit, and they hunt down every uncertainty and resolve it.

- *Debate important things.* Grab a group of smart people and ask them to research a topic. By all means, bring AI-produced research, but then debate your views without your laptop. Debating an idea requires understanding it.

## The Human Touch

What I fear most is that you will stop thinking for yourself. I see some concerning signs of this, and it scares the living shit out of me. If you are asked to write a reflection on your recent personal growth, do not let a fucking AI write it. I can't believe I feel a need to say that, but I do.

- *Put effort into your writing.* When you open a blank document and start typing, there is friction as you generate words. You need to think. You need to try different ways to articulate an idea. That work is valuable. It's how you learn and understand. It shapes your brain. Letting an AI generate the words is dangerous because you read them and think: "Yeah, I would write that," when reality is: you absolutely would not. It's like keeping a book open while doing practice exams. You sneak a glance, and suddenly the test feels easy. But once you're in the exam room without the book, your brain is blank. We all know what cheating feels like when we should be thinking. If you leave an AI-chat-session unable to recollect what you discussed, you are suffering from teflon AI-brain. Watch out for that, I think it's as poisonous as doomscrolling. Real-life example: As a parent, I know the difference between telling a bedtime story and reading one. Telling a good story takes effort. Reading one is easy. That's why we deliberately make up stories together in my family.

- *Develop good judgment and good values.* Values are critical to living a good life. Knowing what is right and wrong in a situation, and having the integrity to act accordingly, is a rare and valuable trait. It compounds. Kognic is betting that human judgment is irreplaceable. I choose to be a specist, favoring humans over machines. I want machines to serve humanity, not the other way around. The trick is being a technology optimist, i.e., embracing more powerful tools, while drawing a clear line: technology is a tool to advance humanity. That takes values and integrity. Make sure you have them.

- *Embrace your inner entropy.* LLMs lead to mean reversion. They sand down all the rough edges and express things in average ways. That's not interesting. Value comes from independent, novel thinking. Ask yourself: what can I add here? What do I believe that others haven't thought about? What have I experienced that shapes my understanding in a rare way?

- *Only then, polish with AI.* Some of you suck at writing reports. I know, I read them. If that's you, then of course, use AI to clean things up. I'd much rather read text that flows naturally. But I can tell when you lean on the AI too much. The best approach is to fine-tune it to your voice and just use it to fix grammar and flow. Don't let the AI polish away your personality.

## Leadership is Telling a Great Story

I've fallen in love with the word legible. It means "clear enough to read." For ideas to gain traction, they must be legible to their audience.

I receive many pitch decks from startups, and being on the other side of that process is revealing. Most founders are so obsessed with their idea that they forget pitching means selling shares to a capital allocator. Ask anyone in sales: the key is to make the prospect talk about their needs, then present your solution in their words. That way, the solution is legible: it connects to what they care about.

Great leaders do the same with their teams. They sense their team members' headspace and adjust their message accordingly. Leadership is, to a large degree, storytelling. And telling a great story is about connecting with a listener.

Of course, there are great storytellers who use stories to distort the truth. That's not what we want at Kognic. We want stories that make great ideas engaging. They are a way to make people care about the truth. Here are two things I think matter when telling great stories:

- *Think in terms of transformation, not information.* A common mistake is treating communication as a matter of clinical information transfer: "If I provide the facts and my conclusions, they'll get it." Wrong. People don't process information that way. To pull listeners in, you need tension. A problem. A before-and-after. The listener needs to feel inside something that moves.

- *Use specific examples.* Abstract ideas without specificity are hard to get excited about. Research shows that concrete, specific language activates sensory and motor cortices â€” not just the language center of the brain. When you say "a customer's LiDAR-camera fusion was labeling a cyclist as a trash bag at 40 meters, and nobody caught it for three months," that's a different experience than "we help improve annotation quality." One paints a picture. The other is background noise.

## The Challenge

Your goal should be to combine these three things: *Critical Thinking* + *The Human Touch* + *Telling a Great Story*.

It's what separates leaders who get things done from leaders who merely have good ideas. We live in an age of token abundance. Text is cheap. Content is everywhere. Anyone can sound articulate with the push of a button. The premium is no longer on being able to write things down.

> The premium is on thinking something worth saying, proving it's true, and making other people feel it in their bones.

That's the superpower no AI can give you.

And it's the one every single person at Kognic needs to develop.
